{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87489833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pcd\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\PCD\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362d2344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 286500 entries, 0 to 286499\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   ts             286500 non-null  int64  \n",
      " 1   userId         286500 non-null  object \n",
      " 2   sessionId      286500 non-null  int64  \n",
      " 3   page           286500 non-null  object \n",
      " 4   auth           286500 non-null  object \n",
      " 5   method         286500 non-null  object \n",
      " 6   status         286500 non-null  int64  \n",
      " 7   level          286500 non-null  object \n",
      " 8   itemInSession  286500 non-null  int64  \n",
      " 9   location       278154 non-null  object \n",
      " 10  userAgent      278154 non-null  object \n",
      " 11  lastName       278154 non-null  object \n",
      " 12  firstName      278154 non-null  object \n",
      " 13  registration   278154 non-null  float64\n",
      " 14  gender         278154 non-null  object \n",
      " 15  artist         228108 non-null  object \n",
      " 16  song           228108 non-null  object \n",
      " 17  length         228108 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(12)\n",
      "memory usage: 39.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_json(\"../data/customer_churn_mini.json\", lines=True)\n",
    "\n",
    "# df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fffced0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = df[df['userId'].notnull()]\n",
    "df = df[df['userId'].astype(str).str.strip() != '']\n",
    "\n",
    "\n",
    "df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "df['registration'] = pd.to_datetime(df['registration'], unit='ms')\n",
    "\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedc458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93147fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn\n",
      "0    0.768889\n",
      "1    0.231111\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define churn label\n",
    "df['churn'] = df['page'].apply(lambda x: 1 if x == 'Cancellation Confirmation' else 0)\n",
    "\n",
    "# Aggregate churn at user level\n",
    "user_churn = df.groupby('userId')['churn'].max().reset_index()\n",
    "\n",
    "print(user_churn['churn'].value_counts(normalize=True))  # See class imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23cc338",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for axis 0 with size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 260\u001b[39m\n\u001b[32m    258\u001b[39m fitted = {}\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mdl \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     fitted_model, metrics = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     fitted[name] = fitted_model\n\u001b[32m    262\u001b[39m     results[name] = metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 241\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(name, model, Xtr, ytr, Xte, yte)\u001b[39m\n\u001b[32m    238\u001b[39m     expanded_cat_names = []\n\u001b[32m    239\u001b[39m feature_names = numeric + expanded_cat_names\n\u001b[32m    240\u001b[39m importances = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     [{\u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m: feature_names[i], \u001b[33m'\u001b[39m\u001b[33mimportance_mean\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mpi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimportances_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m), \u001b[33m'\u001b[39m\u001b[33mimportance_std\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pi.importances_std[i])}\n\u001b[32m    242\u001b[39m      \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(feature_names))],\n\u001b[32m    243\u001b[39m     key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33mimportance_mean\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    244\u001b[39m     reverse=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    245\u001b[39m )\n\u001b[32m    247\u001b[39m results = {\n\u001b[32m    248\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mroc_auc\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(roc),\n\u001b[32m    249\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpr_auc\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(pr_auc),\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtop_features\u001b[39m\u001b[33m'\u001b[39m: importances[:\u001b[32m15\u001b[39m]\n\u001b[32m    255\u001b[39m }\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, results\n",
      "\u001b[31mIndexError\u001b[39m: index 24 is out of bounds for axis 0 with size 24"
     ]
    }
   ],
   "source": [
    "# Step 2 â€” Train & Evaluate the Churn Model (user-level)\n",
    "# ------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Load\n",
    "# -----------------------------\n",
    "# Replace with your real path (CSV or parquet)\n",
    "DATA_PATH = \"../data/customer_churn_mini.json\"\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "\n",
    "# Basic cleaning consistent with Step 1\n",
    "df = df[df['userId'].notnull()]\n",
    "df['userId'] = df['userId'].astype(str).str.strip()\n",
    "df = df[df['userId'] != '']\n",
    "df['ts'] = pd.to_datetime(df['ts'], unit='ms', errors='coerce')\n",
    "if 'registration' in df.columns:\n",
    "    df['registration'] = pd.to_datetime(df['registration'], unit='ms', errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Label definition (user-level)\n",
    "# -----------------------------\n",
    "# Churn = user has at least one \"Cancellation Confirmation\" event\n",
    "df['churn_event'] = (df['page'] == 'Cancellation Confirmation').astype(int)\n",
    "user_labels = df.groupby('userId', as_index=False)['churn_event'].max().rename(columns={'churn_event': 'churn'})\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature engineering (user-level aggregates)\n",
    "#    Keep it leakage-safe: use only info up to the user's last observed timestamp (static snapshot).\n",
    "# -----------------------------\n",
    "# Helpful flags\n",
    "df['is_song'] = (df['page'] == 'NextSong').astype(int)\n",
    "df['thumbs_up'] = (df['page'] == 'Thumbs Up').astype(int)\n",
    "df['thumbs_down'] = (df['page'] == 'Thumbs Down').astype(int)\n",
    "df['add_friend'] = (df['page'] == 'Add Friend').astype(int)\n",
    "df['add_playlist'] = (df['page'] == 'Add to Playlist').astype(int)\n",
    "df['roll_advert'] = (df['page'] == 'Roll Advert').astype(int)\n",
    "df['help'] = (df['page'] == 'Help').astype(int)\n",
    "df['error'] = (df['page'] == 'Error').astype(int)\n",
    "df['submit_upgrade'] = (df['page'] == 'Submit Upgrade').astype(int)\n",
    "df['submit_downgrade'] = (df['page'] == 'Submit Downgrade').astype(int)\n",
    "\n",
    "# Session-level durations: approximate using (max ts - min ts) per (user, session)\n",
    "sess_span = df.groupby(['userId','sessionId'])['ts'].agg(['min','max']).reset_index()\n",
    "sess_span['session_minutes'] = (sess_span['max'] - sess_span['min']).dt.total_seconds() / 60.0\n",
    "user_session_stats = sess_span.groupby('userId').agg(\n",
    "    n_sessions=('sessionId','nunique'),\n",
    "    session_min_avg=('session_minutes','mean'),\n",
    "    session_min_std=('session_minutes','std'),\n",
    "    session_min_max=('session_minutes','max')\n",
    ").reset_index()\n",
    "\n",
    "# Distinct content\n",
    "distinct_stats = df.groupby('userId').agg(\n",
    "    distinct_artists=('artist', lambda s: s.dropna().nunique()),\n",
    "    distinct_songs=('song',   lambda s: s.dropna().nunique()),\n",
    ").reset_index()\n",
    "\n",
    "# Activity counts\n",
    "activity = df.groupby('userId').agg(\n",
    "    n_events=('page','count'),\n",
    "    n_songs=('is_song','sum'),\n",
    "    n_thumbs_up=('thumbs_up','sum'),\n",
    "    n_thumbs_down=('thumbs_down','sum'),\n",
    "    n_add_friend=('add_friend','sum'),\n",
    "    n_add_playlist=('add_playlist','sum'),\n",
    "    n_roll_advert=('roll_advert','sum'),\n",
    "    n_help=('help','sum'),\n",
    "    n_error=('error','sum'),\n",
    "    n_submit_up=('submit_upgrade','sum'),\n",
    "    n_submit_down=('submit_downgrade','sum'),\n",
    ").reset_index()\n",
    "\n",
    "# Ratios (avoid div by zero)\n",
    "def ratio(a, b):\n",
    "    a = a.astype(float); b = b.astype(float)\n",
    "    return np.where(b>0, a/b, 0.0)\n",
    "\n",
    "activity['ratio_up_per_song']   = ratio(activity['n_thumbs_up'], activity['n_songs'])\n",
    "activity['ratio_down_per_song'] = ratio(activity['n_thumbs_down'], activity['n_songs'])\n",
    "activity['ratio_ads_per_event'] = ratio(activity['n_roll_advert'], activity['n_events'])\n",
    "\n",
    "# Level dynamics\n",
    "if 'level' in df.columns:\n",
    "    level_changes = (df.sort_values(['userId','ts'])\n",
    "                       .groupby('userId')['level']\n",
    "                       .apply(lambda s: (s != s.shift()).sum() - 1)\n",
    "                       .reset_index(name='n_level_changes'))\n",
    "else:\n",
    "    level_changes = pd.DataFrame({'userId': df['userId'].unique(), 'n_level_changes': 0})\n",
    "\n",
    "# Time since registration to last activity (proxy for tenure)\n",
    "last_ts = df.groupby('userId')['ts'].max().reset_index(name='last_ts')\n",
    "if 'registration' in df.columns:\n",
    "    reg = df.groupby('userId')['registration'].min().reset_index(name='registration')\n",
    "    tenure = pd.merge(last_ts, reg, on='userId', how='left')\n",
    "    tenure['days_since_registration'] = (tenure['last_ts'] - tenure['registration']).dt.total_seconds() / (3600*24)\n",
    "else:\n",
    "    tenure = last_ts.copy()\n",
    "    tenure['days_since_registration'] = np.nan\n",
    "\n",
    "# User agent (coarse)\n",
    "ua = df.groupby('userId')['userAgent'].agg(lambda s: s.dropna().iloc[-1] if len(s.dropna()) else np.nan).reset_index()\n",
    "def ua_family(u):\n",
    "    if pd.isna(u): return 'Unknown'\n",
    "    u = u.lower()\n",
    "    if 'iphone' in u or 'ios' in u: return 'iOS'\n",
    "    if 'android' in u: return 'Android'\n",
    "    if 'mac os' in u or 'macintosh' in u: return 'Mac'\n",
    "    if 'windows' in u: return 'Windows'\n",
    "    return 'Other'\n",
    "ua['ua_family'] = ua['userAgent'].apply(ua_family)\n",
    "\n",
    "# Gender (last seen)\n",
    "gender = df.groupby('userId')['gender'].agg(lambda s: s.dropna().iloc[-1] if len(s.dropna()) else np.nan).reset_index()\n",
    "\n",
    "# Merge all features\n",
    "features = (activity\n",
    "            .merge(user_session_stats, on='userId', how='left')\n",
    "            .merge(distinct_stats, on='userId', how='left')\n",
    "            .merge(level_changes, on='userId', how='left')\n",
    "            .merge(tenure[['userId','days_since_registration']], on='userId', how='left')\n",
    "            .merge(ua[['userId','ua_family']], on='userId', how='left')\n",
    "            .merge(gender, on='userId', how='left')\n",
    "            .merge(user_labels, on='userId', how='left'))\n",
    "\n",
    "# Replace inf / NaN from stds/ratios\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features.fillna({\n",
    "    'session_min_std': 0.0,\n",
    "    'distinct_artists': 0,\n",
    "    'distinct_songs': 0,\n",
    "    'n_level_changes': 0,\n",
    "    'days_since_registration': 0.0\n",
    "}, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train / Test Split by user\n",
    "# -----------------------------\n",
    "X = features.drop(columns=['churn','userId'])\n",
    "y = features['churn'].astype(int)\n",
    "groups = features['userId']  # ensure no leakage\n",
    "\n",
    "# Identify categorical vs numeric\n",
    "categorical = []\n",
    "if 'ua_family' in X.columns: categorical.append('ua_family')\n",
    "if 'gender' in X.columns: categorical.append('gender')\n",
    "numeric = [c for c in X.columns if c not in categorical]\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num','passthrough', numeric),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Group-wise split preserving class balance as much as possible\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Models\n",
    "# -----------------------------\n",
    "# Baseline: Logistic Regression (with class_weight)\n",
    "logreg = Pipeline(steps=[\n",
    "    ('preprocess', pre),\n",
    "    ('clf', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=200,\n",
    "        n_jobs=None,\n",
    "        solver='liblinear'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Tree model: RandomForest (robust baseline)\n",
    "rf = Pipeline(steps=[\n",
    "    ('preprocess', pre),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'logreg': logreg,\n",
    "    'random_forest': rf\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Train & Evaluate\n",
    "# -----------------------------\n",
    "def evaluate(name, model, Xtr, ytr, Xte, yte):\n",
    "    model.fit(Xtr, ytr)\n",
    "    y_proba = model.predict_proba(Xte)[:,1] if hasattr(model[-1], \"predict_proba\") else model.decision_function(Xte)\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(yte, y_proba)\n",
    "    pr_auc = average_precision_score(yte, y_proba)\n",
    "    f1 = f1_score(yte, y_pred)\n",
    "    prec, rec, _, _ = precision_recall_fscore_support(yte, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(yte, y_pred).tolist()\n",
    "\n",
    "    # Permutation importance on test set for quick error analysis\n",
    "    # (works on the final, post-preprocess pipeline)\n",
    "    pi = permutation_importance(model, Xte, yte, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    # Map importances back to feature names (numeric + one-hot cats)\n",
    "    # ColumnTransformer + OHE can expand columns; we extract names:\n",
    "    ohe = model.named_steps['preprocess'].named_transformers_['cat']\n",
    "    cat_cols = categorical\n",
    "    if len(cat_cols) and hasattr(ohe, 'get_feature_names_out'):\n",
    "        expanded_cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "    else:\n",
    "        expanded_cat_names = []\n",
    "    feature_names = numeric + expanded_cat_names\n",
    "    importances = sorted(\n",
    "        [{'feature': feature_names[i], 'importance_mean': float(pi.importances_mean[i]), 'importance_std': float(pi.importances_std[i])}\n",
    "         for i in range(len(feature_names))],\n",
    "        key=lambda x: x['importance_mean'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'roc_auc': float(roc),\n",
    "        'pr_auc': float(pr_auc),\n",
    "        'f1': float(f1),\n",
    "        'precision': float(prec),\n",
    "        'recall': float(rec),\n",
    "        'confusion_matrix': cm,\n",
    "        'top_features': importances[:15]\n",
    "    }\n",
    "    return model, results\n",
    "\n",
    "fitted = {}\n",
    "for name, mdl in models.items():\n",
    "    fitted_model, metrics = evaluate(name, mdl, X_train, y_train, X_test, y_test)\n",
    "    fitted[name] = fitted_model\n",
    "    results[name] = metrics\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Pick the model to deploy (by PR-AUC, then ROC-AUC)\n",
    "# -----------------------------\n",
    "def pick_model(result_dict):\n",
    "    ranked = sorted(\n",
    "        result_dict.items(),\n",
    "        key=lambda kv: (kv[1]['pr_auc'], kv[1]['roc_auc']),\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked[0][0]\n",
    "\n",
    "best_name = pick_model(results)\n",
    "best_model = fitted[best_name]\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Persist artifacts for deployment/retraining\n",
    "# -----------------------------\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(best_model, f\"artifacts/model_{best_name}.joblib\")\n",
    "meta = {\n",
    "    'best_model': best_name,\n",
    "    'metrics': results[best_name],\n",
    "    'all_results': results,\n",
    "    'feature_columns': {\n",
    "        'numeric': numeric,\n",
    "        'categorical': categorical\n",
    "    }\n",
    "}\n",
    "with open(\"artifacts/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Best model: {best_name}\")\n",
    "print(json.dumps(results[best_name], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2873e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
